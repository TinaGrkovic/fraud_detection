{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "from IPython.display import clear_output\n",
    "display(HTML(\"<style>.container { width:90% }</style>\"))\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import statsmodels.api as sample_data\n",
    "import random\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn import ensemble\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# For Random Oversampling\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from collections import Counter\n",
    "\n",
    "# NN\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Sample of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded a random 10% sample (100000 rows) successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define file path in Google Drive (Update the path as per your directory structure)\n",
    "file_path = \"BAN6025Project2Data.csv\"\n",
    "#file_path = \"/content/drive/My Drive/Copy of BAN6025Project2Data.csv\"\n",
    "\n",
    "# Set sample size\n",
    "sample_size = 100000  # 10% of 1,000,000 rows\n",
    "\n",
    "# Step 1: Get total row count (without loading full dataset)\n",
    "with open(file_path, 'r') as f:\n",
    "    total_rows = sum(1 for line in f) - 1  # Subtract 1 for header/\n",
    "\n",
    "# Step 2: Generate random row indices to skip (excluding header)\n",
    "skip_rows = sorted(random.sample(range(1, total_rows + 1), total_rows - sample_size))\n",
    "\n",
    "# Step 3: Load only the sampled rows\n",
    "df_sample = pd.read_csv(file_path, skiprows=skip_rows)\n",
    "\n",
    "\n",
    "print(f\"Loaded a random 10% sample ({len(df_sample)} rows) successfully.\")\n",
    "df = df_sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make X25 into 0,1 because it only has 2 values\n",
    "df['X25'] = df['X25'].replace({'INTERNET': 0, 'TELEAPP': 1}).astype(int)\n",
    "\n",
    "# Make into categories: Target_Y, X17, X19, X20, X22, X24, X25, X28\n",
    "df['Target_Y'] = df['Target_Y'].astype('category')\n",
    "'''\n",
    "df['X17'] = df['X17'].astype('category')\n",
    "df['X19'] = df['X19'].astype('category')\n",
    "df['X20'] = df['X20'].astype('category')\n",
    "df['X22'] = df['X22'].astype('category')\n",
    "df['X24'] = df['X24'].astype('category')\n",
    "df['X25'] = df['X25'].astype('category')\n",
    "df['X28'] = df['X28'].astype('category')\n",
    "'''\n",
    "# Drop col X30 because it only has 0's\n",
    "df = df.drop(['X30'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make into dummies: X8, X15, X18, X27 - (they are not numbers)\n",
    "X = df.drop(['Target_Y'], axis=1)\n",
    "X = pd.get_dummies(X, columns=['X8', 'X15', 'X18', 'X27', 'X1', 'X5', 'X14', 'X21', 'X23', 'X29', 'X31'], drop_first=True, dtype='float')\n",
    "\n",
    "\n",
    "y = df['Target_Y']\n",
    "\n",
    "# Apply SMOTE with Limited Oversampling (1% â†’ 10% instead of full balance)\n",
    "smote = SMOTE(sampling_strategy=0.1, random_state=42)  # Increase minority class to 10%\n",
    "X, y = smote.fit_resample(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -- train set -- \n",
      "Accuracy : 0.9923\n",
      "Precision: 0.9697\n",
      "Recall.  : 0.9458\n",
      "F1 Score.  : 0.9576\n",
      "\n",
      " -- test set -- \n",
      "Accuracy : 0.9872\n",
      "Precision: 0.9548\n",
      "Recall.  : 0.8984\n",
      "F1 Score.  : 0.9257\n"
     ]
    }
   ],
   "source": [
    "# create an instance of a random forest classifier using default values\n",
    "# n_estimators': 150, 'max_depth': 25, 'min_samples_leaf': 5, 'max_features': 'sqrt'\n",
    "# 'n_estimators': 50, 'max_depth': 25, 'min_samples_split': 20, 'min_samples_leaf': 5, 'max_features': 'sqrt', 'class_weight': 'balanced'\n",
    "# n_estimators': 100, 'max_depth': 25, 'min_samples_split': 20, 'min_samples_leaf': 5, 'max_features': 6, 'class_weight': 'balanced_subsample'\n",
    "rf = RandomForestClassifier( n_estimators=100, max_depth=25, min_samples_split=20, min_samples_leaf=5, max_features=6, class_weight='balanced_subsample', n_jobs=-1, random_state=42 )\n",
    "\n",
    "# fit the model to the training data\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the training and test data\n",
    "y_pred_train_rf = rf.predict(X_train)\n",
    "y_pred_test_rf = rf.predict(X_test)\n",
    "\n",
    "y_prob_train_rf = rf.predict_proba(X_train)\n",
    "y_prob_test_rf = rf.predict_proba(X_test)\n",
    "\n",
    "# calculate the accuracy, precision, and recall scores\n",
    "acc_train = accuracy_score(y_train, y_pred_train_rf)\n",
    "prec_train = precision_score(y_train, y_pred_train_rf)\n",
    "rec_train = recall_score(y_train, y_pred_train_rf)\n",
    "f1_train = f1_score(y_train, y_pred_train_rf)\n",
    "\n",
    "\n",
    "# print the scores\n",
    "print(\" -- train set -- \")\n",
    "print(\"Accuracy : {:.4f}\".format(acc_train))\n",
    "print(\"Precision: {:.4f}\".format(prec_train))\n",
    "print(\"Recall.  : {:.4f}\".format(rec_train))\n",
    "print(\"F1 Score.  : {:.4f}\".format(f1_train))\n",
    "print(\"\")\n",
    "\n",
    "# calculate the accuracy, precision, and recall scores\n",
    "acc_test = accuracy_score(y_test, y_pred_test_rf)\n",
    "prec_test = precision_score(y_test, y_pred_test_rf)\n",
    "rec_test = recall_score(y_test, y_pred_test_rf)\n",
    "f1_test = f1_score(y_test, y_pred_test_rf)\n",
    "\n",
    "print(\" -- test set -- \")\n",
    "print(\"Accuracy : {:.4f}\".format(acc_test))\n",
    "print(\"Precision: {:.4f}\".format(prec_test))\n",
    "print(\"Recall.  : {:.4f}\".format(rec_test))\n",
    "print(\"F1 Score.  : {:.4f}\".format(f1_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-06 21:59:37,557] A new study created in memory with name: no-name-8386660f-d072-4c37-9a3e-c52ea7acd821\n",
      "[I 2025-02-06 21:59:46,215] Trial 0 finished with value: 0.7678430418624586 and parameters: {'n_estimators': 200, 'max_depth': 5, 'min_samples_split': 30, 'min_samples_leaf': 10, 'max_features': 6, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.7678430418624586.\n",
      "[I 2025-02-06 21:59:50,560] Trial 1 finished with value: 0.8501837042366572 and parameters: {'n_estimators': 50, 'max_depth': 20, 'min_samples_split': 20, 'min_samples_leaf': 15, 'max_features': 2, 'class_weight': 'balanced_subsample'}. Best is trial 1 with value: 0.8501837042366572.\n",
      "[I 2025-02-06 22:00:07,615] Trial 2 finished with value: 0.8876258458449917 and parameters: {'n_estimators': 250, 'max_depth': 20, 'min_samples_split': 40, 'min_samples_leaf': 25, 'max_features': 6, 'class_weight': 'balanced_subsample'}. Best is trial 2 with value: 0.8876258458449917.\n",
      "[I 2025-02-06 22:00:19,198] Trial 3 finished with value: 0.873807690791471 and parameters: {'n_estimators': 300, 'max_depth': 15, 'min_samples_split': 10, 'min_samples_leaf': 20, 'max_features': 4, 'class_weight': 'balanced'}. Best is trial 2 with value: 0.8876258458449917.\n",
      "[I 2025-02-06 22:00:26,519] Trial 4 finished with value: 0.8603456146391824 and parameters: {'n_estimators': 150, 'max_depth': 10, 'min_samples_split': 40, 'min_samples_leaf': 5, 'max_features': 6, 'class_weight': 'balanced'}. Best is trial 2 with value: 0.8876258458449917.\n",
      "[I 2025-02-06 22:00:29,461] Trial 5 finished with value: 0.8700647574859864 and parameters: {'n_estimators': 50, 'max_depth': 15, 'min_samples_split': 40, 'min_samples_leaf': 15, 'max_features': 4, 'class_weight': 'balanced_subsample'}. Best is trial 2 with value: 0.8876258458449917.\n",
      "[I 2025-02-06 22:00:43,719] Trial 6 finished with value: 0.8586196618572997 and parameters: {'n_estimators': 300, 'max_depth': 10, 'min_samples_split': 20, 'min_samples_leaf': 10, 'max_features': 6, 'class_weight': 'balanced'}. Best is trial 2 with value: 0.8876258458449917.\n",
      "[I 2025-02-06 22:00:51,829] Trial 7 finished with value: 0.8295335937653593 and parameters: {'n_estimators': 300, 'max_depth': 10, 'min_samples_split': 30, 'min_samples_leaf': 10, 'max_features': 2, 'class_weight': 'balanced'}. Best is trial 2 with value: 0.8876258458449917.\n",
      "[I 2025-02-06 22:01:00,712] Trial 8 finished with value: 0.766212522593091 and parameters: {'n_estimators': 100, 'max_depth': 5, 'min_samples_split': 40, 'min_samples_leaf': 25, 'max_features': 'sqrt', 'class_weight': 'balanced'}. Best is trial 2 with value: 0.8876258458449917.\n",
      "[I 2025-02-06 22:01:07,657] Trial 9 finished with value: 0.8496963618665069 and parameters: {'n_estimators': 150, 'max_depth': 20, 'min_samples_split': 50, 'min_samples_leaf': 25, 'max_features': 2, 'class_weight': 'balanced'}. Best is trial 2 with value: 0.8876258458449917.\n",
      "[I 2025-02-06 22:01:40,748] Trial 10 finished with value: 0.8853206739825544 and parameters: {'n_estimators': 250, 'max_depth': 25, 'min_samples_split': 50, 'min_samples_leaf': 30, 'max_features': 'sqrt', 'class_weight': 'balanced_subsample'}. Best is trial 2 with value: 0.8876258458449917.\n",
      "[I 2025-02-06 22:02:04,839] Trial 11 finished with value: 0.8853206739825544 and parameters: {'n_estimators': 250, 'max_depth': 25, 'min_samples_split': 50, 'min_samples_leaf': 30, 'max_features': 'sqrt', 'class_weight': 'balanced_subsample'}. Best is trial 2 with value: 0.8876258458449917.\n",
      "[I 2025-02-06 22:02:27,504] Trial 12 finished with value: 0.8853206739825544 and parameters: {'n_estimators': 250, 'max_depth': 25, 'min_samples_split': 50, 'min_samples_leaf': 30, 'max_features': 'sqrt', 'class_weight': 'balanced_subsample'}. Best is trial 2 with value: 0.8876258458449917.\n",
      "[I 2025-02-06 22:02:40,636] Trial 13 finished with value: 0.8878445148656816 and parameters: {'n_estimators': 200, 'max_depth': 20, 'min_samples_split': 40, 'min_samples_leaf': 25, 'max_features': 6, 'class_weight': 'balanced_subsample'}. Best is trial 13 with value: 0.8878445148656816.\n",
      "[I 2025-02-06 22:02:53,077] Trial 14 finished with value: 0.8878445148656816 and parameters: {'n_estimators': 200, 'max_depth': 20, 'min_samples_split': 40, 'min_samples_leaf': 25, 'max_features': 6, 'class_weight': 'balanced_subsample'}. Best is trial 13 with value: 0.8878445148656816.\n",
      "[I 2025-02-06 22:03:05,797] Trial 15 finished with value: 0.8830878702917363 and parameters: {'n_estimators': 200, 'max_depth': 15, 'min_samples_split': 30, 'min_samples_leaf': 20, 'max_features': 6, 'class_weight': 'balanced_subsample'}. Best is trial 13 with value: 0.8878445148656816.\n",
      "[I 2025-02-06 22:03:18,618] Trial 16 finished with value: 0.8865947601702154 and parameters: {'n_estimators': 150, 'max_depth': 20, 'min_samples_split': 30, 'min_samples_leaf': 25, 'max_features': 6, 'class_weight': 'balanced_subsample'}. Best is trial 13 with value: 0.8878445148656816.\n",
      "[I 2025-02-06 22:03:33,327] Trial 17 finished with value: 0.8945064999807443 and parameters: {'n_estimators': 200, 'max_depth': 20, 'min_samples_split': 40, 'min_samples_leaf': 20, 'max_features': 6, 'class_weight': 'balanced_subsample'}. Best is trial 17 with value: 0.8945064999807443.\n",
      "[I 2025-02-06 22:03:40,628] Trial 18 finished with value: 0.8961083483369107 and parameters: {'n_estimators': 100, 'max_depth': 25, 'min_samples_split': 20, 'min_samples_leaf': 20, 'max_features': 6, 'class_weight': 'balanced_subsample'}. Best is trial 18 with value: 0.8961083483369107.\n",
      "[I 2025-02-06 22:03:46,421] Trial 19 finished with value: 0.887467343082683 and parameters: {'n_estimators': 100, 'max_depth': 25, 'min_samples_split': 10, 'min_samples_leaf': 20, 'max_features': 4, 'class_weight': 'balanced_subsample'}. Best is trial 18 with value: 0.8961083483369107.\n",
      "[I 2025-02-06 22:03:53,170] Trial 20 finished with value: 0.9027528979479875 and parameters: {'n_estimators': 100, 'max_depth': 25, 'min_samples_split': 20, 'min_samples_leaf': 15, 'max_features': 6, 'class_weight': 'balanced_subsample'}. Best is trial 20 with value: 0.9027528979479875.\n",
      "[I 2025-02-06 22:03:59,932] Trial 21 finished with value: 0.9027528979479875 and parameters: {'n_estimators': 100, 'max_depth': 25, 'min_samples_split': 20, 'min_samples_leaf': 15, 'max_features': 6, 'class_weight': 'balanced_subsample'}. Best is trial 20 with value: 0.9027528979479875.\n",
      "[I 2025-02-06 22:04:06,460] Trial 22 finished with value: 0.9027528979479875 and parameters: {'n_estimators': 100, 'max_depth': 25, 'min_samples_split': 20, 'min_samples_leaf': 15, 'max_features': 6, 'class_weight': 'balanced_subsample'}. Best is trial 20 with value: 0.9027528979479875.\n",
      "[I 2025-02-06 22:04:12,793] Trial 23 finished with value: 0.9027528979479875 and parameters: {'n_estimators': 100, 'max_depth': 25, 'min_samples_split': 20, 'min_samples_leaf': 15, 'max_features': 6, 'class_weight': 'balanced_subsample'}. Best is trial 20 with value: 0.9027528979479875.\n",
      "[I 2025-02-06 22:04:16,233] Trial 24 finished with value: 0.8990658172549155 and parameters: {'n_estimators': 50, 'max_depth': 25, 'min_samples_split': 10, 'min_samples_leaf': 15, 'max_features': 6, 'class_weight': 'balanced_subsample'}. Best is trial 20 with value: 0.9027528979479875.\n",
      "[I 2025-02-06 22:04:23,319] Trial 25 finished with value: 0.92484073562495 and parameters: {'n_estimators': 100, 'max_depth': 25, 'min_samples_split': 20, 'min_samples_leaf': 5, 'max_features': 6, 'class_weight': 'balanced_subsample'}. Best is trial 25 with value: 0.92484073562495.\n",
      "[I 2025-02-06 22:04:27,441] Trial 26 finished with value: 0.923017595543421 and parameters: {'n_estimators': 50, 'max_depth': 25, 'min_samples_split': 20, 'min_samples_leaf': 5, 'max_features': 6, 'class_weight': 'balanced_subsample'}. Best is trial 25 with value: 0.92484073562495.\n",
      "[I 2025-02-06 22:04:32,834] Trial 27 finished with value: 0.8805455771989049 and parameters: {'n_estimators': 50, 'max_depth': 20, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 2, 'class_weight': 'balanced_subsample'}. Best is trial 25 with value: 0.92484073562495.\n",
      "[I 2025-02-06 22:04:35,956] Trial 28 finished with value: 0.9183664137132626 and parameters: {'n_estimators': 50, 'max_depth': 25, 'min_samples_split': 20, 'min_samples_leaf': 5, 'max_features': 4, 'class_weight': 'balanced_subsample'}. Best is trial 25 with value: 0.92484073562495.\n",
      "[I 2025-02-06 22:04:38,096] Trial 29 finished with value: 0.736421108166378 and parameters: {'n_estimators': 50, 'max_depth': 5, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 4, 'class_weight': 'balanced_subsample'}. Best is trial 25 with value: 0.92484073562495.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'n_estimators': 100, 'max_depth': 25, 'min_samples_split': 20, 'min_samples_leaf': 5, 'max_features': 6, 'class_weight': 'balanced_subsample'}\n",
      "Best Cross-Validation F1 Score: 0.92484073562495\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Define the objective function for Bayesian Optimization\n",
    "def objective(trial):\n",
    "    # Define hyperparameter search space\n",
    "    n_estimators = trial.suggest_int('n_estimators', 50, 300, step=50)\n",
    "    max_depth = trial.suggest_int('max_depth', 5, 25, step=5)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 10, 50, step=10)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 5, 30, step=5)\n",
    "    max_features = trial.suggest_categorical('max_features', ['sqrt', 2, 4, 6])\n",
    "    class_weight = trial.suggest_categorical('class_weight', ['balanced', 'balanced_subsample'])\n",
    "\n",
    "    # Initialize Random Forest with trial-selected hyperparameters\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        max_features=max_features,\n",
    "        class_weight=class_weight,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Perform Cross-Validation\n",
    "    score = cross_val_score(rf, X_train, y_train, cv=3, scoring='f1', n_jobs=-1).mean()\n",
    "    return score  # Maximizing F1-score\n",
    "\n",
    "# Create and run Bayesian Optimization study\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=30)  # Run 30 trials to find best params\n",
    "\n",
    "# Print Best Parameters & Score\n",
    "print(\"Best Parameters:\", study.best_params)\n",
    "print(\"Best Cross-Validation F1 Score:\", study.best_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -- train set -- \n",
      "Accuracy : 0.9994\n",
      "Precision: 1.0000\n",
      "Recall.  : 0.9931\n",
      "F1 Score.  : 0.9966\n",
      "\n",
      " -- test set -- \n",
      "Accuracy : 0.9898\n",
      "Precision: 0.9908\n",
      "Recall.  : 0.8928\n",
      "F1 Score.  : 0.9393\n"
     ]
    }
   ],
   "source": [
    "# create an instance of the Gradient Boosting Classifier using default values\n",
    "# n_estimators': 150, 'learning_rate': 0.15000000000000002, 'max_depth': 9, 'min_samples_split': 20, 'min_samples_leaf': 15, 'subsample': 1.0, 'max_features': 'sqrt'\n",
    "gb = GradientBoostingClassifier(n_estimators = 150, learning_rate = 0.15000000000000002, max_depth=9,min_samples_split=20, min_samples_leaf=15,subsample=1.0, max_features='sqrt', random_state=904)\n",
    "\n",
    "# fit the model to the training data\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the training and test data\n",
    "y_pred_train_gb = gb.predict(X_train)\n",
    "y_pred_test_gb = gb.predict(X_test)\n",
    "\n",
    "y_prob_train_gb = gb.predict_proba(X_train)\n",
    "y_prob_test_gb = gb.predict_proba(X_test)\n",
    "\n",
    "# calculate the accuracy, precision, and recall scores\n",
    "acc_train = accuracy_score(y_train, y_pred_train_gb)\n",
    "prec_train = precision_score(y_train, y_pred_train_gb)\n",
    "rec_train = recall_score(y_train, y_pred_train_gb)\n",
    "f1_train = f1_score(y_train, y_pred_train_gb)\n",
    "\n",
    "# print the scores\n",
    "print(\" -- train set -- \")\n",
    "print(\"Accuracy : {:.4f}\".format(acc_train))\n",
    "print(\"Precision: {:.4f}\".format(prec_train))\n",
    "print(\"Recall.  : {:.4f}\".format(rec_train))\n",
    "print(\"F1 Score.  : {:.4f}\".format(f1_train))\n",
    "print(\"\")\n",
    "\n",
    "# calculate the accuracy, precision, and recall scores\n",
    "acc_test = accuracy_score(y_test, y_pred_test_gb)\n",
    "prec_test = precision_score(y_test, y_pred_test_gb)\n",
    "rec_test = recall_score(y_test, y_pred_test_gb)\n",
    "f1_test = f1_score(y_test, y_pred_test_gb)\n",
    "\n",
    "print(\" -- test set -- \")\n",
    "print(\"Accuracy : {:.4f}\".format(acc_test))\n",
    "print(\"Precision: {:.4f}\".format(prec_test))\n",
    "print(\"Recall.  : {:.4f}\".format(rec_test))\n",
    "print(\"F1 Score.  : {:.4f}\".format(f1_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GB Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-06 22:19:39,028] A new study created in memory with name: no-name-b9cce6c3-1db8-4e07-98a6-a601c42a43b4\n",
      "[I 2025-02-06 22:19:46,968] Trial 0 finished with value: 0.9258368652719251 and parameters: {'n_estimators': 50, 'learning_rate': 0.2, 'max_depth': 6, 'min_samples_split': 10, 'min_samples_leaf': 15, 'subsample': 0.8999999999999999, 'max_features': 'log2'}. Best is trial 0 with value: 0.9258368652719251.\n",
      "[I 2025-02-06 22:23:07,912] Trial 1 finished with value: 0.932949410744025 and parameters: {'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': 6, 'min_samples_split': 20, 'min_samples_leaf': 20, 'subsample': 0.8999999999999999, 'max_features': None}. Best is trial 1 with value: 0.932949410744025.\n",
      "[I 2025-02-06 22:25:08,177] Trial 2 finished with value: 0.9260338114476264 and parameters: {'n_estimators': 100, 'learning_rate': 0.3, 'max_depth': 6, 'min_samples_split': 30, 'min_samples_leaf': 10, 'subsample': 0.7, 'max_features': None}. Best is trial 1 with value: 0.932949410744025.\n",
      "[I 2025-02-06 22:25:13,771] Trial 3 finished with value: 0.928629756861915 and parameters: {'n_estimators': 100, 'learning_rate': 0.3, 'max_depth': 3, 'min_samples_split': 30, 'min_samples_leaf': 10, 'subsample': 0.8999999999999999, 'max_features': 'log2'}. Best is trial 1 with value: 0.932949410744025.\n",
      "[I 2025-02-06 22:27:41,319] Trial 4 finished with value: 0.9332019580562355 and parameters: {'n_estimators': 100, 'learning_rate': 0.2, 'max_depth': 6, 'min_samples_split': 40, 'min_samples_leaf': 10, 'subsample': 0.8999999999999999, 'max_features': None}. Best is trial 4 with value: 0.9332019580562355.\n",
      "[I 2025-02-06 22:29:33,838] Trial 5 finished with value: 0.9319869569305826 and parameters: {'n_estimators': 50, 'learning_rate': 0.3, 'max_depth': 9, 'min_samples_split': 40, 'min_samples_leaf': 20, 'subsample': 0.8999999999999999, 'max_features': None}. Best is trial 4 with value: 0.9332019580562355.\n",
      "[I 2025-02-06 22:29:48,563] Trial 6 finished with value: 0.930022799789145 and parameters: {'n_estimators': 50, 'learning_rate': 0.25, 'max_depth': 12, 'min_samples_split': 40, 'min_samples_leaf': 10, 'subsample': 0.8999999999999999, 'max_features': 'log2'}. Best is trial 4 with value: 0.9332019580562355.\n",
      "[I 2025-02-06 22:30:25,102] Trial 7 finished with value: 0.9276094739645369 and parameters: {'n_estimators': 50, 'learning_rate': 0.25, 'max_depth': 3, 'min_samples_split': 30, 'min_samples_leaf': 20, 'subsample': 0.8999999999999999, 'max_features': None}. Best is trial 4 with value: 0.9332019580562355.\n",
      "[I 2025-02-06 22:30:52,812] Trial 8 finished with value: 0.9341102031073871 and parameters: {'n_estimators': 100, 'learning_rate': 0.2, 'max_depth': 12, 'min_samples_split': 30, 'min_samples_leaf': 20, 'subsample': 0.8999999999999999, 'max_features': 'sqrt'}. Best is trial 8 with value: 0.9341102031073871.\n",
      "[I 2025-02-06 22:36:50,028] Trial 9 finished with value: 0.9358020238203825 and parameters: {'n_estimators': 150, 'learning_rate': 0.3, 'max_depth': 9, 'min_samples_split': 40, 'min_samples_leaf': 20, 'subsample': 1.0, 'max_features': None}. Best is trial 9 with value: 0.9358020238203825.\n",
      "[I 2025-02-06 22:37:28,526] Trial 10 finished with value: 0.9331301906477305 and parameters: {'n_estimators': 150, 'learning_rate': 0.05, 'max_depth': 9, 'min_samples_split': 10, 'min_samples_leaf': 5, 'subsample': 1.0, 'max_features': 'sqrt'}. Best is trial 9 with value: 0.9358020238203825.\n",
      "[I 2025-02-06 22:38:13,320] Trial 11 finished with value: 0.9358628893023608 and parameters: {'n_estimators': 150, 'learning_rate': 0.15000000000000002, 'max_depth': 12, 'min_samples_split': 20, 'min_samples_leaf': 15, 'subsample': 1.0, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.9358628893023608.\n",
      "[I 2025-02-06 22:39:09,400] Trial 12 finished with value: 0.9354981755570715 and parameters: {'n_estimators': 150, 'learning_rate': 0.1, 'max_depth': 12, 'min_samples_split': 20, 'min_samples_leaf': 15, 'subsample': 1.0, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.9358628893023608.\n",
      "[I 2025-02-06 22:39:50,698] Trial 13 finished with value: 0.9360475074873122 and parameters: {'n_estimators': 150, 'learning_rate': 0.15000000000000002, 'max_depth': 9, 'min_samples_split': 20, 'min_samples_leaf': 15, 'subsample': 1.0, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.9360475074873122.\n",
      "[I 2025-02-06 22:40:33,271] Trial 14 finished with value: 0.9354569725805545 and parameters: {'n_estimators': 150, 'learning_rate': 0.15000000000000002, 'max_depth': 12, 'min_samples_split': 20, 'min_samples_leaf': 15, 'subsample': 0.7999999999999999, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.9360475074873122.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'n_estimators': 150, 'learning_rate': 0.15000000000000002, 'max_depth': 9, 'min_samples_split': 20, 'min_samples_leaf': 15, 'subsample': 1.0, 'max_features': 'sqrt'}\n",
      "Best Cross-Validation F1 Score: 0.9360475074873122\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Define the objective function for Bayesian Optimization\n",
    "def objective(trial):\n",
    "    # Define hyperparameter search space\n",
    "    n_estimators = trial.suggest_int('n_estimators', 50, 150, step=50)  # Reduced upper limit\n",
    "    learning_rate = trial.suggest_float('learning_rate', 0.05, 0.3, step=0.05)  # Skipped very low values\n",
    "    max_depth = trial.suggest_int('max_depth', 3, 12, step=3)  # Capped at 12 to reduce complexity\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 10, 40, step=10)  # Reduced range\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 5, 20, step=5)  # Reduced range\n",
    "    subsample = trial.suggest_float('subsample', 0.7, 1.0, step=0.1)  # Keeps randomness for generalization\n",
    "    max_features = trial.suggest_categorical('max_features', ['sqrt', 'log2', None])\n",
    "\n",
    "    # Initialize Gradient Boosting with trial-selected hyperparameters\n",
    "    gb = GradientBoostingClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        learning_rate=learning_rate,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        subsample=subsample,\n",
    "        max_features=max_features,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Perform Cross-Validation (cv=2 instead of cv=3 for speed)\n",
    "    score = cross_val_score(gb, X_train, y_train, cv=2, scoring='f1', n_jobs=-1).mean()\n",
    "    return score  # Maximizing F1-score\n",
    "\n",
    "# Create and run Bayesian Optimization study (n_trials=15 instead of 30)\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=15)  # Reduced trials for faster execution\n",
    "\n",
    "# Print Best Parameters & Score\n",
    "print(\"Best Parameters:\", study.best_params)\n",
    "print(\"Best Cross-Validation F1 Score:\", study.best_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
